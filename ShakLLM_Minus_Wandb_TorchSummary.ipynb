{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpLF0OXquJ3k",
        "outputId": "c35fd8fd-fcf1-4278-f856-fa3c7650e378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-05-25 16:41:51--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-05-25 16:41:51 (17.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaI5ZRfZyQDV",
        "outputId": "0adba282-5723-43a0-acb1-b28fe8e9dabc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GYDHZ-M0bbzM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF_rD9hyl2f8",
        "outputId": "4f9cd325-ed91-4df0-ce7f-a620f5cb78f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1115394\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOEIPhFDmQHk",
        "outputId": "a97729cf-d4a2-4d31-cb81-c2795c4b5ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "vocabulary = sorted(list(set(text)))\n",
        "vocab_size = len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J7LJAQumiEq",
        "outputId": "1a5f051f-7510-4201-98a6-0ef064cb1c91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[20, 47, 1, 58, 46, 43, 56, 43, 2]\n",
            "Hi there!\n"
          ]
        }
      ],
      "source": [
        "vocab_to_int = {v:i for i, v in enumerate(vocabulary)}\n",
        "int_to_vocab = {i:v for i, v in enumerate(vocabulary)}\n",
        "\n",
        "encode = lambda x: [vocab_to_int[i] for i in x]\n",
        "decode = lambda x: \"\".join([int_to_vocab[i] for i in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iSdwPp1woc9X"
      },
      "outputs": [],
      "source": [
        "dataset = torch.tensor(encode(text), dtype = torch.long)\n",
        "train_data = dataset[:int(0.9 * len(dataset))]\n",
        "val_data = dataset[int(0.9 * len(dataset)): int(0.97 * len(dataset))]\n",
        "test_data = dataset[int(0.97 * len(dataset)): ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qJBPfBJXptaQ"
      },
      "outputs": [],
      "source": [
        "class ShaksphereDataLoader(torch.utils.data.DataLoader):\n",
        "    def __init__(self, dataset, seq_len, batch_size):\n",
        "        self.dataset = dataset\n",
        "        self.seq_len = seq_len\n",
        "        self.batch_size = batch_size\n",
        "        self.sub_seq = (len(dataset) // self.seq_len)\n",
        "        self.num_batches = self.sub_seq // self.batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_batches\n",
        "\n",
        "    def __iter__(self):\n",
        "        offsets = torch.randperm(len(self.dataset) - self.seq_len)[:self.sub_seq]\n",
        "        inputs = torch.stack([self.dataset[i: i + self.seq_len] for i in offsets])\n",
        "        outputs = torch.stack([self.dataset[i + 1: i + 1 + self.seq_len] for i in offsets])\n",
        "        rem = inputs.shape[0] % self.batch_size\n",
        "        if rem != 0:\n",
        "            inputs = inputs[:-rem, :].reshape(self.num_batches, self.batch_size, self.seq_len)\n",
        "            outputs = outputs[:-rem, :].reshape(self.num_batches, self.batch_size, self.seq_len)\n",
        "        else:\n",
        "            inputs = inputs.reshape(self.num_batches, self.batch_size, self.seq_len)\n",
        "            outputs = outputs.reshape(self.num_batches, self.batch_size, self.seq_len)\n",
        "\n",
        "        batch_idx = 0\n",
        "        while batch_idx < self.num_batches:\n",
        "            yield inputs[batch_idx, : , :].to(device), outputs[batch_idx, :, :].to(device)\n",
        "            batch_idx = batch_idx + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SKZxT-Qdpv_8"
      },
      "outputs": [],
      "source": [
        "ModelConfig = {\n",
        "    'seq_len': 256,\n",
        "    \"batch_size\": 128,\n",
        "    'embed_dim': 512,\n",
        "    'qk_dim': 512\n",
        "}\n",
        "\n",
        "seq_len = ModelConfig['seq_len']\n",
        "batch_size = ModelConfig['batch_size']\n",
        "embed_dim = ModelConfig['embed_dim']\n",
        "qk_dim = ModelConfig['qk_dim']\n",
        "value_dim = ModelConfig['embed_dim']\n",
        "model_size = ModelConfig['embed_dim']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMsRDgUeqH6Z",
        "outputId": "58fe215c-92e5-4ccb-debd-48de9086a7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "train_loader = ShaksphereDataLoader(train_data, seq_len = ModelConfig['seq_len'], batch_size = ModelConfig['batch_size'])\n",
        "val_loader = ShaksphereDataLoader(val_data, seq_len = ModelConfig['seq_len'], batch_size = ModelConfig['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5V5Vm0xqIrA",
        "outputId": "03971f20-627f-4ea9-e9a0-53ff8cd8a5c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 256])\n",
            "torch.Size([128, 256])\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "isKBuKgG-FDa"
      },
      "outputs": [],
      "source": [
        "class MaskedAttention(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.query_layer = torch.nn.Linear(embed_dim, embed_dim, bias = False)\n",
        "        self.key_layer = torch.nn.Linear(embed_dim, qk_dim, bias = False)\n",
        "        self.value_layer = torch.nn.Linear(embed_dim, value_dim, bias = False)\n",
        "        self.mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n",
        "\n",
        "    def forward(self, tok_embeddings, return_attention_weights = False):\n",
        "        B, T, C = tok_embeddings.shape\n",
        "        Q = self.query_layer(tok_embeddings) # (B, T, qk)\n",
        "        K = self.key_layer(tok_embeddings)   # (B, T, qk)\n",
        "        V = self.value_layer(tok_embeddings) # (B, T, v)\n",
        "\n",
        "        affinities = (Q @ K.transpose(-1, -2)) * K.shape[-1] ** -0.5 # (B, T, T)\n",
        "        affinities = affinities.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
        "\n",
        "        attention_weights = F.softmax(affinities, dim = -1) # (B, T, T)\n",
        "\n",
        "        if return_attention_weights:\n",
        "            return attention_weights @ V, attention_weights\n",
        "        return attention_weights @ V  # (B, T, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CTIEGo7Y-w63"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = head_size\n",
        "        self.mini_head_size = int(self.head_size / self.num_heads)\n",
        "\n",
        "        self.query = torch.nn.Linear(embed_dim, self.head_size)\n",
        "        self.key = torch.nn.Linear(embed_dim, self.head_size)\n",
        "        self.value = torch.nn.Linear(embed_dim, self.head_size)\n",
        "        self.up_project = torch.nn.Linear(self.num_heads * self.mini_head_size, head_size)\n",
        "        self.mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n",
        "\n",
        "    def forward(self, tok_embeddings, return_attention_weights = False):\n",
        "        B, T, C = tok_embeddings.shape\n",
        "        Q = self.query(tok_embeddings)\n",
        "        K = self.key(tok_embeddings)\n",
        "        V = self.value(tok_embeddings)\n",
        "\n",
        "        # Reshape into N sub heads for parallel processing\n",
        "        mini_Q = Q.view(B, T, self.num_heads, self.mini_head_size).permute(0, 2, 1, 3) # (B, nh, T, mini_head)\n",
        "        mini_K = K.view(B, T, self.num_heads, self.mini_head_size).permute(0, 2, 1, 3) # (B, nh, T, mini_head)\n",
        "        mini_V = V.view(B, T, self.num_heads, self.mini_head_size).permute(0, 2, 1, 3) # (B, nh, T, mini_head)\n",
        "\n",
        "        # Scaled dot Product\n",
        "        affinities = (mini_Q @ mini_K.transpose(-1, -2)) * (self.mini_head_size ** -0.5) #  (B, nh, T, T)\n",
        "        # Makes it causal decoder\n",
        "        affinities = affinities.masked_fill(self.mask[:T, :T] == 0, float(\"-inf\"))\n",
        "        attention_weights = F.softmax(affinities, dim = -1) # (B, nh, T, T)\n",
        "\n",
        "        # weighted vlaues for each head\n",
        "        mini_head_weighted_values = attention_weights @ mini_V # (B, nh, T, mini_head)\n",
        "        # Concatenating each mini_head outputs\n",
        "        mini_head_weighted_values = mini_head_weighted_values.permute(0, 2, 1, 3)\n",
        "        head_weighted_values = mini_head_weighted_values.reshape(B, T, self.mini_head_size * self.num_heads) # (B, T, head_size)\n",
        "\n",
        "        if return_attention_weights:\n",
        "            return self.up_project(head_weighted_values), attention_weights\n",
        "        return self.up_project(head_weighted_values) # (B, T, head_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eO3PQEIhA1x"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5-pl_c4m-1Vv"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(torch.nn.Module):\n",
        "    def __init__(self, num_heads):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=model_size)\n",
        "        self.layer_norm_1 = torch.nn.LayerNorm(model_size)\n",
        "        self.linear_1 = torch.nn.Linear(model_size, model_size)\n",
        "        self.linear_2 = torch.nn.Linear(model_size, model_size)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.layer_norm_2 = torch.nn.LayerNorm(model_size)\n",
        "        self.dropout_1 = torch.nn.Dropout(0.1)\n",
        "        self.dropout_2 = torch.nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, embeddings, return_attention_weights = False):\n",
        "        # tokens ---> (B, T, embed_size)\n",
        "        B, T, C = embeddings.shape\n",
        "\n",
        "        # Attention\n",
        "        if return_attention_weights:\n",
        "            weighted_values, attention_weights = self.attention.forward(embeddings, return_attention_weights)\n",
        "        else:\n",
        "            weighted_values = self.attention.forward(embeddings) # (B, T, embed_dim)\n",
        "\n",
        "        # Attention Norm + dropout\n",
        "        weighted_values_drp = self.dropout_1(weighted_values)\n",
        "        norm_values = self.layer_norm_1(weighted_values_drp + embeddings) # (B, T, embed_dim)\n",
        "\n",
        "        # FFN + dropout\n",
        "        linear_1 = self.linear_1(norm_values)\n",
        "        act_vals = self.relu(linear_1)\n",
        "        linear_2 = self.linear_2(act_vals)\n",
        "        linear_2_drp = self.dropout_2(linear_2)\n",
        "\n",
        "        # LayerNorm\n",
        "        ffn_norm = self.layer_norm_2(norm_values + linear_2_drp) # (B, T, embed_dim)\n",
        "\n",
        "\n",
        "        if return_attention_weights:\n",
        "            return ffn_norm, attention_weights\n",
        "        return ffn_norm # (B, T, embed_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PuQgAyTc_pc4"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_embedding = torch.nn.Embedding(seq_len, embed_dim)\n",
        "        self.dropout_0 = torch.nn.Dropout(0.1)\n",
        "        self.decoder_block_1 = DecoderBlock(num_heads=8)\n",
        "        self.decoder_block_2 = DecoderBlock(num_heads=8)\n",
        "        self.decoder_block_3 = DecoderBlock(num_heads=8)\n",
        "\n",
        "        self.projection = torch.nn.Linear(model_size, vocab_size)\n",
        "        self.projection.weight = self.embedding_layer.weight\n",
        "\n",
        "    def forward(self, tokens, return_attention_weights = False):\n",
        "        B, T = tokens.shape\n",
        "        # Token and Positional Embeddings + dropout\n",
        "        tok_embs = self.embedding_layer(tokens) # (B, T, embed_dim)\n",
        "        pos_embs = self.positional_embedding(torch.arange(T).to(device))\n",
        "        pos_tok_embs = tok_embs + pos_embs # (B, T, embed_dim)\n",
        "        pos_tok_embs_drp = self.dropout_0(pos_tok_embs)\n",
        "\n",
        "        # Decoder Blocks\n",
        "        decoder_1 = self.decoder_block_1.forward(pos_tok_embs)\n",
        "        decoder_2 = self.decoder_block_2.forward(decoder_1)\n",
        "        if return_attention_weights:\n",
        "            decoder_3, last_layer_attention_weights = self.decoder_block_3.forward(decoder_2, return_attention_weights=True)\n",
        "        else:\n",
        "            decoder_3 = self.decoder_block_3.forward(decoder_2)\n",
        "\n",
        "        # projection layer\n",
        "        logits = self.projection(decoder_3) # (B, T, vocab_size)\n",
        "\n",
        "        if return_attention_weights:\n",
        "            return logits, last_layer_attention_weights\n",
        "        return logits # (B, T, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEHbjeU5ABFv",
        "outputId": "1d50a095-72d4-4775-98bd-5765f054d374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LanguageModel()\n",
        "model.to(device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ShakGPT_3_512.pt', map_location = device))\n",
        "#summary(model, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qrGYZSzCDDqH"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "scheduler =  torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=4) #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GU636Z0UDHdE"
      },
      "outputs": [],
      "source": [
        "optimizer.param_groups[0]['lr'] = 5e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kqegbFApDP-t"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model, train_loader, val_loader, optimizer, criterion):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.epoch = 0\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        train_loss = 0\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for x, y in self.train_loader:\n",
        "            logits, attention_weights = self.model.forward(x, return_attention_weights=True)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            y = y.view(B*T)\n",
        "            loss = self.criterion(logits, y)\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "\n",
        "        epoch_loss = total_loss / len(train_loader)\n",
        "        self.epoch = self.epoch + 1\n",
        "        print(f\"Epoch: {self.epoch}\\nTrain Loss: {epoch_loss}, Train Perplexity: {np.exp(epoch_loss)}, LR: {self.optimizer.param_groups[0]['lr']}\")\n",
        "        return epoch_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        for x, y in self.val_loader:\n",
        "            logits = self.model.forward(x)\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            y = y.view(B*T)\n",
        "            val_loss = self.criterion(logits, y)\n",
        "            total_loss = total_loss + val_loss.item()\n",
        "\n",
        "        epoch_loss = total_loss / len(self.val_loader)\n",
        "        print(f'Validation Loss: {epoch_loss}')\n",
        "        return epoch_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, max_tokens, prompt, temperature = 1, top_p = 1):\n",
        "        self.model.eval()\n",
        "        for i in range(max_tokens):\n",
        "            logits = self.model.forward(prompt)\n",
        "            logit = logits[:, -1, :] # (B, C)\n",
        "            logit = logit / temperature\n",
        "            probs = F.softmax(logit, dim = -1)\n",
        "            weighted_probs = self.topPTransform(probs, top_p)\n",
        "            #token = torch.argmax(probs, dim = -1).view(-1, 1)\n",
        "            token = torch.multinomial(weighted_probs, num_samples = 1) # (B, 1)\n",
        "            prompt = torch.cat((prompt, token), dim = -1) # (B, T + 1)\n",
        "        return prompt\n",
        "\n",
        "    def topPTransform(self, probs, top_p):\n",
        "        probs_sorted_vals, probs_sort_idx = torch.sort(probs, descending=True)\n",
        "        prob_cumsum = torch.cumsum(probs_sorted_vals, dim = -1)\n",
        "\n",
        "        absolute_diff = torch.abs(prob_cumsum - top_p)\n",
        "        closest_index = torch.argmin(absolute_diff).item()\n",
        "        idx_to_remove = probs_sort_idx[:, closest_index + 1:]\n",
        "\n",
        "        mask = torch.ones_like(probs)\n",
        "        mask[:, idx_to_remove] = 0\n",
        "\n",
        "        probs = probs * mask\n",
        "        weighted_probs = probs / torch.sum(probs, dim = -1)\n",
        "        return weighted_probs\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.model.state_dict(), path)\n",
        "        print(\"Model saved at \" + path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "Qqm-5c90D32q",
        "outputId": "21df1cea-8a66-417e-9a20-639bcfad51ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 8.601622295379638, Train Perplexity: 5440.478499124308, LR: 5e-05\n",
            "Validation Loss: 4.496948003768921\n",
            "Model saved at /content/drive/MyDrive/ShakGPT_3_512.pt\n",
            "Generation: BARNARDO: Who's there?\n",
            "FRANCISCO: Nay, answer me. Stand and unfold yourself.\n",
            "BARNARDO:      e              e                                  e                      ee                e e                                                     e  ee            \n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3c63f5ad0b9d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#scheduler.step(val_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-92dfdbb3e096>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "trainer = Trainer(model = model, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, criterion = criterion)\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(epochs):\n",
        "    train_loss = trainer.train()\n",
        "    val_loss = trainer.validate()\n",
        "    #scheduler.step(val_loss)\n",
        "    if val_loss < best_val_loss:\n",
        "        trainer.save('/content/drive/MyDrive/ShakGPT_3_512.pt')\n",
        "        best_val_loss = val_loss\n",
        "    print(\"Generation: \" + decode(trainer.generate(prompt =\n",
        "                            torch.tensor(encode(\"BARNARDO: Who's there?\\nFRANCISCO: Nay, answer me. Stand and unfold yourself.\\nBARNARDO:\")).view(1, -1).to(device),\n",
        "                            max_tokens=170,\n",
        "                            top_p = 0.7)[0].tolist()))\n",
        "    print('\\n')\n",
        "    if epoch == 700:\n",
        "        optimizer.param_groups[0]['lr'] = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ERe1SAmG7Fg"
      },
      "outputs": [],
      "source": [
        "from random import uniform\n",
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def topPTransform(probs, top_p):\n",
        "        probs_sorted_vals, probs_sort_idx = torch.sort(probs, descending=True)\n",
        "        prob_cumsum = torch.cumsum(probs_sorted_vals, dim = -1)\n",
        "\n",
        "        absolute_diff = torch.abs(prob_cumsum - top_p)\n",
        "        closest_index = torch.argmin(absolute_diff).item()\n",
        "        idx_to_remove = probs_sort_idx[:, closest_index + 1:]\n",
        "\n",
        "        mask = torch.ones_like(probs)\n",
        "        mask[:, idx_to_remove] = 0\n",
        "\n",
        "        probs = probs * mask\n",
        "        weighted_probs = probs / torch.sum(probs, dim = -1)\n",
        "        return weighted_probs\n",
        "\n",
        "def persistent_generation(prompt, max_tokens, temperature = 1, top_p = 1):\n",
        "    generated_text = prompt\n",
        "    model.eval()\n",
        "    tokens = torch.tensor(encode(prompt)).reshape(1, -1).to(device)\n",
        "    buffer = 10\n",
        "    token_count = 0\n",
        "    while token_count < max_tokens:\n",
        "        if tokens.shape[-1] >= seq_len:\n",
        "            tokens = tokens[:, tokens.shape[-1] - seq_len + 10: ]\n",
        "        logits = model.forward(tokens)\n",
        "        logit = logits[:, -1, :] # (B, C)\n",
        "\n",
        "        # temperature\n",
        "        logit = logit / temperature\n",
        "\n",
        "        # top_p\n",
        "        probs = F.softmax(logit, dim = -1) # (B, C)\n",
        "        weighted_probs = topPTransform(probs, top_p)\n",
        "        predicted_token = torch.multinomial(weighted_probs, num_samples = 1) # (B, 1)\n",
        "        generated_text = generated_text + decode(predicted_token[0].cpu().detach().tolist())\n",
        "        tokens = torch.cat((tokens, predicted_token), dim = -1) # (B, T + 1) # (1, 1)\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        print(generated_text)\n",
        "        token_count = token_count + 1\n",
        "\n",
        "print(persistent_generation(\"ACT I\\n\\nSCENE I. Elsinore. A platform before the Castle.\\n\\n\\nEnter Francisco and Barnardo, two sentinels.\", max_tokens = 700, top_p = 0.7))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
